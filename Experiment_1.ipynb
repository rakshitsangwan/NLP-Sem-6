{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX2PxMHHbxC1",
        "outputId": "1454ab4a-16ac-445a-d2d0-0ffd20bade03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your text: I love Python\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer , SpaceTokenizer\n",
        "nltk.download('punkt')\n",
        "text = input(\"Enter your text: \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokenization:\")\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFsaauOOcDTG",
        "outputId": "d2914ae9-fb60-4dcd-bc67-f0efb6193101"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:\n",
            "['I', 'love', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uonyRkqcFln",
        "outputId": "1bcccc5b-d994-4ee1-f070-63723ecb7e3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['I', 'love', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "space_tokenizer = SpaceTokenizer()\n",
        "space_tokens = space_tokenizer.tokenize(text)\n",
        "print(\"Space Tokenization:\")\n",
        "print(space_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-pHmA8qcIU0",
        "outputId": "b0f8d812-69be-4e86-9225-93bf25655225"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Space Tokenization:\n",
            "['I', 'love', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter text:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ac7-35FcLTm",
        "outputId": "87eb5375-3024-4770-df89-1ef20e71b6f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter text:this is a coding @app @Delhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokens = regexp_tokenize(text, pattern=r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "print(\"\\nRegExp Tokenization:\")\n",
        "print(regexp_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-vdS_ypcWNL",
        "outputId": "0c39fd38-fb56-473b-d205-19912f020a9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RegExp Tokenization:\n",
            "['this', 'is', 'a', 'coding', '@app', '@Delhi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"this is a coding @app @Delhi\"\n",
        "pattern = r'\\w+|\\S'\n",
        "regexp_tokens = re.findall(pattern, text)"
      ],
      "metadata": {
        "id": "Fy0B9Yx3cYkx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(regexp_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SFJ9j8jcf3G",
        "outputId": "5e9daacb-cb31-4a74-d59d-e310faf031e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'coding', '@', 'app', '@', 'Delhi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"@here is good #deal\"\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG7DpQhAch74",
        "outputId": "91529241-0268-4876-b5ca-c0c314dc4334"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['@here', 'is', 'good', '#deal']\n"
          ]
        }
      ]
    }
  ]
}